{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6465b65a-aa1d-4c2c-915e-ae552adbb34d",
   "metadata": {},
   "source": [
    "![Cosmos-Reason1-7B](cosmos-reason1_banner.png)\n",
    "[Github: Cosmos-Reason1-7B](https://github.com/nvidia-cosmos/cosmos-reason1/tree/main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51968787-03b0-437b-bb48-eef50cea24ba",
   "metadata": {},
   "source": [
    "### Set Up Your New Instance in a Terminal\n",
    "To open a terminal: Launcher tab -> Other -> Terminal\n",
    "\n",
    "```bash\n",
    "# Install uv/just\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "source $HOME/.local/bin/env\n",
    "uv tool install rust-just\n",
    "\n",
    "# Clone the repository\n",
    "git clone https://github.com/nvidia-cosmos/cosmos-reason1.git\n",
    "cd cosmos-reason1\n",
    "\n",
    "# Install the package using venv\n",
    "just install\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Make sure pip is installed in the .venv\n",
    "python -m ensurepip --upgrade\n",
    "\n",
    "# Restart the venv\n",
    "deactivate\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Set up a custom kernel for Jupyter Notebook\n",
    "pip3 install ipykernel\n",
    "python -m ipykernel install --user --name=reason1 --display-name \"Python (.venv) Reason1\"\n",
    "pip3 install -U ipywidgets\n",
    "\n",
    "# Login your Huggingface account to download checkpoints later\n",
    "# Get your token here: https://huggingface.co/settings/tokens\n",
    "pip3 install huggingface_hub\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086edfb-6aa5-41d0-a506-058e07ba0ec4",
   "metadata": {},
   "source": [
    "### Switch to the Custom Python Kernel\n",
    "1. Go back to reason1.ipynb\n",
    "2. click on the **Python3(ipykernel)** on upper-right corner\n",
    "3. Pick **Python(.venv)Reason1**, then click Select button. (If you don't see the option, try restaring the notebook.)\n",
    "4. The upper-right kernel button should be changed to Python(.venv)Reason1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb80bc-e8ad-4197-9016-b8ad9dfdf08c",
   "metadata": {},
   "source": [
    "### Download Models from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ab84a1-b733-4dcc-88c3-ccea24eaa28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6320d7c46b4e0b8dda1805d10a6ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/nvidia/Cosmos-Reason1-7B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"nvidia/Cosmos-Reason1-7B\",\n",
    "    local_dir=\"nvidia/Cosmos-Reason1-7B\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a82cf9-9a2b-4b6d-9aec-230a817e206c",
   "metadata": {},
   "source": [
    "## Test Inference\n",
    "It takes a few seconds to start printing outputs. You should see \"<\\/answer>\" at the end of the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779f118f-96b7-46f7-ac6c-16f0e159ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 09:18:17 [__init__.py:244] Automatically detected platform cuda.\n",
      "nvidia/Cosmos-Reason1-7B\n",
      "INFO 07-29 09:18:25 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-29 09:18:25 [config.py:1472] Using max model len 128000\n",
      "INFO 07-29 09:18:26 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 07-29 09:18:27 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 07-29 09:18:30 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-29 09:18:32 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-29 09:18:32 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='nvidia/Cosmos-Reason1-7B', speculative_config=None, tokenizer='nvidia/Cosmos-Reason1-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=nvidia/Cosmos-Reason1-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-29 09:18:33 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-29 09:18:34 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-29 09:18:34 [gpu_model_runner.py:1770] Starting to load model nvidia/Cosmos-Reason1-7B...\n",
      "INFO 07-29 09:18:34 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-29 09:18:34 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.43it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.53it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.25it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.27it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 09:18:38 [default_loader.py:272] Loading weights took 3.21 seconds\n",
      "INFO 07-29 09:18:38 [gpu_model_runner.py:1801] Model loading took 15.6271 GiB and 3.542802 seconds\n",
      "INFO 07-29 09:18:38 [gpu_model_runner.py:2238] Encoder cache will be initialized with a budget of 98304 tokens, and profiled with 1 video items of the maximum feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 09:18:48 [backends.py:508] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/7c2ad702c1/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-29 09:18:48 [backends.py:519] Dynamo bytecode transform time: 4.88 s\n",
      "INFO 07-29 09:18:52 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 3.210 s\n",
      "INFO 07-29 09:18:52 [monitor.py:34] torch.compile takes 4.88 s in total\n",
      "INFO 07-29 09:18:53 [gpu_worker.py:232] Available KV cache memory: 49.57 GiB\n",
      "INFO 07-29 09:18:53 [kv_cache_utils.py:716] GPU KV cache size: 928,176 tokens\n",
      "INFO 07-29 09:18:53 [kv_cache_utils.py:720] Maximum concurrency for 128,000 tokens per request: 7.25x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:15<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-29 09:19:09 [gpu_model_runner.py:2326] Graph capturing finished in 15 secs, took 0.66 GiB\n",
      "INFO 07-29 09:19:09 [core.py:172] init engine (profile, create kv cache, warmup model) took 30.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "qwen-vl-utils using torchvision to read video.\n",
      "/home/ubuntu/cosmos-reason1/.venv/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a71074f0f7454aa24785725d692145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2d684978d24ade808e22393c893a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking if it's safe to turn right based on the video provided. First, I need to parse the details given. The scenario is a residential street with parked cars and driveways. The driver has stopped at an intersection, checked for pedestrians and traffic, and now wants to turn right.\n",
      "\n",
      "The key points here are the parked cars and driveways. When turning right, especially in residential areas, visibility can be an issue. Parked cars might block the view of oncoming traffic or pedestrians stepping out from between them. Also, there could be vehicles exiting driveways without checking properly. The driver already checked both directions, but after that, when actually making the turn, they need to be cautious again.\n",
      "\n",
      "In the video, there's mention of multiple parked cars along the curb and driveways. That setup increases the risk of hidden hazards. Even though the driver stopped and looked before proceeding, during the turn itself, they should slow down, check blind spots, and maybe come to a complete stop again if necessary. In many places, you're required to yield to any potential traffic coming from the left when turning right, especially if there's no traffic signal. So even if there was none at the initial stop, once moving, the driver must ensure the path is clear.\n",
      "\n",
      "Additionally, the presence of driveways means that cars backing out might not see the approaching vehicle. The driver needs to watch for cars reversing out of driveways while turning. Since there are several parked cars, the area is likely narrow, so the turning radius matters. If the road is too tight, the driver might not have enough space to maneuver safely.\n",
      "\n",
      "The original video shows the driver \"proceeds cautiously\" after checking. But the question is whether it's safe. The answer would depend on whether all precautions are taken. If the driver checks again, slows down, and yields appropriately, then it's safe. However, if they proceed without further checks despite the parked cars and driveways, it could be risky.\n",
      "\n",
      "So, putting it all together: It's generally advisable to turn right slowly, check mirrors, look for pedestrians, and be prepared to stop if something unexpected appears. Given the parked cars and driveways shown, extra caution is needed. Therefore, the answer would probably say that while it's possible to turn right safely after checking, the presence of obstacles requires heightened vigilance.\n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "Based on the shown scenario, **it is generally unsafe to proceed immediately** with a right turn without additional caution, despite the initial checks shown. Here's why:\n",
      "\n",
      "1. **Obstructions from parked cars/driveways**: Vehicles parked along curbs or in driveways may obscure pedestrians, cyclists, or other vehicles emerging unexpectedly. Drivers must account for blind spots and hidden hazards when turning.\n",
      "\n",
      "2. **Residential context**: Residential areas often have lower speed limits and unpredictable traffic (e.g., children, pets, or vehicles pulling out of driveways). Even after stopping, drivers should maintain reduced speed and be ready to yield to unseen risks.\n",
      "\n",
      "3. **Lack of traffic signals**: The absence of traffic lights or signs means the driver must rely entirely on right-of-way rules (typically yielding to pedestrians and oncoming traffic) and self-assess for safety.\n",
      "\n",
      "**Recommended steps**: \n",
      "- Slow down further before the turn.\n",
      "- Scan the intersection again for pedestrians, cyclists, or vehicles (especially near driveways).\n",
      "- Be prepared to stop if anything emerges suddenly.\n",
      "\n",
      "**Conclusion**: While the driver initially checked for traffic and pedestrians, the presence of obstructions and the residential setting necessitate extra vigilance. Proceeding cautiously, rather than aggressively, would enhance safety.\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# You can also replace the MODEL_PATH by a safetensors folder path mentioned above\n",
    "MODEL_PATH = \"nvidia/Cosmos-Reason1-7B\"\n",
    "VIDEO_PATH = \"cosmos-reason1/assets/sample.mp4\"\n",
    "\n",
    "print(MODEL_PATH)\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.05,\n",
    "    max_tokens=4096,\n",
    ")\n",
    "\n",
    "video_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question in the following format: <think>\\nyour reasoning\\n</think>\\n\\n<answer>\\nyour answer\\n</answer>.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": (\n",
    "                    \"Is it safe to turn right?\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"video\", \n",
    "                \"video\": VIDEO_PATH,\n",
    "                \"fps\": 4,\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Here we use video messages as a demonstration\n",
    "messages = video_messages\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "\n",
    "mm_data = {}\n",
    "if image_inputs is not None:\n",
    "    mm_data[\"image\"] = image_inputs\n",
    "if video_inputs is not None:\n",
    "    mm_data[\"video\"] = video_inputs\n",
    "\n",
    "llm_inputs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"multi_modal_data\": mm_data,\n",
    "\n",
    "    # FPS will be returned in video_kwargs\n",
    "    \"mm_processor_kwargs\": video_kwargs,\n",
    "}\n",
    "\n",
    "outputs = llm.generate([llm_inputs], sampling_params=sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv) Reason1",
   "language": "python",
   "name": "reason1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6465b65a-aa1d-4c2c-915e-ae552adbb34d",
   "metadata": {},
   "source": [
    "![Cosmos-Reason1-7B](cosmos-reason1_banner.png)\n",
    "\n",
    "**Cosmos-Reason1** is a suite of models, tools, and benchmarks designed to enable multimodal large language models (LLMs) to reason with physical common sense and generate grounded responses. This notebook helps you set up the environment and demonstrates two example inference use cases.\n",
    "\n",
    "The following steps are based on [Github: Cosmos-Reason1-7B](https://github.com/nvidia-cosmos/cosmos-reason1/tree/main)\n",
    "- Tested Commits:\n",
    "    - GitHub Commit ID: 98ebb68dfe2aaa1af2c78308e774b65f6b0ecd3d\n",
    "    - Huggingface Commit ID: 8fe96c1fa10db9e666b6fa6a87fea57dd9635649"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae35b1-521e-40e8-b413-947208775c99",
   "metadata": {},
   "source": [
    "### Create the requirements.txt\n",
    "---\n",
    "This creates a requirements.txt file with all the necessary Python packages needed to run Cosmos-Reason1 models and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace8a69-4cd3-400e-8b25-6be8e0b6fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "accelerate\n",
    "qwen-vl-utils\n",
    "rich\n",
    "torch\n",
    "torchcodec\n",
    "torchvision\n",
    "transformers>=4.51.3\n",
    "vllm\n",
    "\n",
    "ipykernel\n",
    "ipywidgets\n",
    "huggingface_hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51968787-03b0-437b-bb48-eef50cea24ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Setup Environment and Dependencies\n",
    "---\n",
    "Execute the following commands in a terminal. To open a terminal: Launcher tab -> Other -> Terminal\n",
    " \n",
    "```bash\n",
    "# Download the sample video file\n",
    "wget https://github.com/nvidia-cosmos/cosmos-reason1/raw/98ebb68dfe2aaa1af2c78308e774b65f6b0ecd3d/assets/sample.mp4\n",
    " \n",
    "# Install uv package manager\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "source $HOME/.local/bin/env\n",
    " \n",
    "# Login your Huggingface account to download checkpoints later\n",
    "# Get your access token here: https://huggingface.co/settings/tokens\n",
    "uv tool install -U \"huggingface_hub[cli]\"\n",
    "hf auth login\n",
    " \n",
    "# Create a python virtual environment and install dependencies\n",
    "uv venv\n",
    "source .venv/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "# Create a pyhton kernel for the notebook\n",
    "python -m ipykernel install --user --name=reason1 --display-name \"Python (.venv) Reason1\"\n",
    " \n",
    "# Restart the python venv\n",
    "deactivate\n",
    "source .venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086edfb-6aa5-41d0-a506-058e07ba0ec4",
   "metadata": {},
   "source": [
    "### Switch to the Custom Python Kernel\n",
    "---\n",
    "1. Go back to the notebook: *cosmos-reason1.ipynb*\n",
    "2. click on the **Python3(ipykernel)** on upper-right corner\n",
    "3. Pick **Python(.venv)Reason1** in *Start python Kernel* section, then click Select button. (If you don't see the option, try restaring the notebook.)\n",
    "4. The upper-right kernel button should be updated to *Python(.venv)Reason1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb80bc-e8ad-4197-9016-b8ad9dfdf08c",
   "metadata": {},
   "source": [
    "### Download Checkpoints from Huggingface\n",
    "---\n",
    "You should see the following message when all files have been downloaded successfully.\n",
    "```bash\n",
    "'/home/ubuntu/nvidia/Cosmos-Reason1-7B'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab84a1-b733-4dcc-88c3-ccea24eaa28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(\n",
    "    repo_id=\"nvidia/Cosmos-Reason1-7B\",\n",
    "    local_dir=\"nvidia/Cosmos-Reason1-7B\",\n",
    "    revision=\"8fe96c1fa10db9e666b6fa6a87fea57dd9635649\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d444281-7907-4a9f-a10e-27cda36fca32",
   "metadata": {},
   "source": [
    "### Use Case #1: Transformers\n",
    "---\n",
    "This Python script performs multimodal inference using the Cosmos-Reason1-7B model. It processes a combination of a text prompt, image files, and video files, then generates and prints a textual response based on the visual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e870e-d99e-401a-a35b-dc3ead199c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qwen_vl_utils\n",
    "import transformers\n",
    "from rich import print\n",
    "\n",
    "# --- Parameters to configure ---\n",
    "prompt = \"Please describe the video.\"\n",
    "images = []  # TODO: Optional, replace with your actual image path(s)\n",
    "videos = [\"sample.mp4\"]  # TODO: Optional, replace with your video path(s) if any\n",
    "\n",
    "# You can leave these as default or change them\n",
    "model_name = \"nvidia/Cosmos-Reason1-7B\"\n",
    "system_prompt = \"You are a helpful assistant. Answer the question in the following format: <think>\\nyour reasoning\\n</think>\\n\\n<answer>\\nyour answer\\n</answer>.\"\n",
    "fps = 1  # Downsample video frame rate\n",
    "max_pixels = 81920  # Downsample media max pixels\n",
    "# --- End of configuration ---\n",
    "\n",
    "# 1. Process the media and text inputs\n",
    "user_content = []\n",
    "for image in images or []:\n",
    "    user_content.append(\n",
    "        {\"type\": \"image\", \"image\": image, \"max_pixels\": max_pixels}\n",
    "    )\n",
    "for video in videos or []:\n",
    "    user_content.append(\n",
    "        {\"type\": \"video\", \"video\": video, \"fps\": fps, \"max_pixels\": max_pixels}\n",
    "    )\n",
    "user_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "# 2. Format the messages for the model\n",
    "messages = []\n",
    "if system_prompt:\n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "print(\"Messages:\", messages)\n",
    "\n",
    "# 3. Load the model and processor\n",
    "model = transformers.Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor: transformers.Qwen2_5_VLProcessor = (\n",
    "    transformers.AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    ")\n",
    "\n",
    "# 4. Set up generation configuration\n",
    "generation_config = transformers.GenerationConfig(\n",
    "    do_sample=True,\n",
    "    max_new_tokens=4096,\n",
    "    repetition_penalty=1.05,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "# 5. Process the messages into tensors\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = qwen_vl_utils.process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# 6. Run inference\n",
    "generated_ids = model.generate(**inputs, generation_config=generation_config)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :]\n",
    "    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False,\n",
    ")\n",
    "\n",
    "# 7. Print the final output\n",
    "print(f\"\\n\\n{output_text[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216781a-f4ef-4b31-afcd-3b2980c9d794",
   "metadata": {},
   "source": [
    "### Use Case #2: vLLM\n",
    "---\n",
    "This script uses the VLLM library for high-performance multimodal inference with the Cosmos-Reason1-7B model. It processes a video file (sample.mp4) along with a text question, and then generates and prints a textual answer based on the video's content.\n",
    "\n",
    "It takes a few seconds to start printing outputs. You should see \"<\\/answer>\" at the end of the log.\n",
    "\n",
    "NOTE: If you encounter *RuntimeError: CUDA error*, try upgrading GPU driver to 570.172.08 (CUDA = 12.8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# --- Parameters to configure ---\n",
    "VIDEO_PATH = \"sample.mp4\"  # TODO: Optional, replace with your video path\n",
    "MODEL_PATH = \"nvidia/Cosmos-Reason1-7B\"\n",
    "# --- End of configuration ---\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.05,\n",
    "    max_tokens=4096,\n",
    ")\n",
    "\n",
    "video_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. Answer the question in the following format: <think>\\nyour reasoning\\n</think>\\n\\n<answer>\\nyour answer\\n</answer>.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": (\"Is it safe to turn right?\")},\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": VIDEO_PATH,\n",
    "                \"fps\": 4,\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Here we use video messages as a demonstration\n",
    "messages = video_messages\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "image_inputs, video_inputs, video_kwargs = process_vision_info(\n",
    "    messages, return_video_kwargs=True\n",
    ")\n",
    "\n",
    "mm_data = {}\n",
    "if image_inputs is not None:\n",
    "    mm_data[\"image\"] = image_inputs\n",
    "if video_inputs is not None:\n",
    "    mm_data[\"video\"] = video_inputs\n",
    "\n",
    "llm_inputs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"multi_modal_data\": mm_data,\n",
    "    # FPS will be returned in video_kwargs\n",
    "    \"mm_processor_kwargs\": video_kwargs,\n",
    "}\n",
    "\n",
    "outputs = llm.generate([llm_inputs], sampling_params=sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv) Reason1",
   "language": "python",
   "name": "reason1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

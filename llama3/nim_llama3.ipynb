{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2e8b5a-a716-4866-8e73-7c03127e98af",
   "metadata": {},
   "source": [
    "![Notebook Banner](llama3_banner.png)\n",
    "\n",
    "## Set Up Docker Login (Executing on A Terminal Locally)\n",
    "\n",
    "Replace <your api key> with your own NGC API Key\n",
    "\n",
    "```bash\n",
    "export NGC_CLI_API_KEY=\"nvapi-xxx\"\n",
    "echo \"$NGC_CLI_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc68f3-4d9a-4322-ac35-7599eeb981e7",
   "metadata": {},
   "source": [
    "## Deploy NIM (Executing on this Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf8bc0-2d24-4ffd-a737-b54de30eef9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "036841bc33b195208da4220918b06ed3bf31e4d0407db53ac346a5d230a8c18c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export NGC_API_KEY=\"nvapi-xxx\"\n",
    "\n",
    "export CONTAINER_NAME=\"Llama3-8B-Instruct\"\n",
    "export IMG_NAME=\"nvcr.io/nim/meta/llama3-8b-instruct:1.0.3\"\n",
    "export LOCAL_NIM_CACHE=~/.cache/nim\n",
    "mkdir -p $LOCAL_NIM_CACHE\n",
    "\n",
    "docker run -d --rm --name=$CONTAINER_NAME \\\n",
    "    --runtime=nvidia \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "    -u $(id -u) \\\n",
    "    -p 8000:8000 \\\n",
    "    $IMG_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a0c684-d0cc-424f-aba3-5725993f2d36",
   "metadata": {},
   "source": [
    "### Make Sure the Contianer is Up and Running\n",
    "You should see this line in the log.\n",
    "\n",
    "INFO 07-30 10:39:50.289 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c409cd6-a1af-4589-8283-62d8c1928712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "== NVIDIA Inference Microservice LLM NIM ==\n",
      "===========================================\n",
      "\n",
      "NVIDIA Inference Microservice LLM NIM Version 1.0.3\n",
      "Model: meta/llama3-8b-instruct\n",
      "\n",
      "Container image Copyright (c) 2016-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This NIM container is governed by the NVIDIA AI Product Agreement here:\n",
      "https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/.\n",
      "A copy of this license can be found under /opt/nim/LICENSE.\n",
      "\n",
      "The use of this model is governed by the AI Foundation Models Community License\n",
      "here: https://docs.nvidia.com/ai-foundation-models-community-license.pdf.\n",
      "\n",
      "ADDITIONAL INFORMATION: Meta Llama 3 Community License, Built with Meta Llama 3.\n",
      "A copy of the Llama 3 license can be found under /opt/nim/MODEL_LICENSE.\n",
      "\n",
      "2025-07-30 10:39:35,710 [INFO] PyTorch version 2.2.2 available.\n",
      "2025-07-30 10:39:36,342 [WARNING] [TRT-LLM] [W] Logger level already set from environment. Discard new verbosity: error\n",
      "2025-07-30 10:39:36,342 [INFO] [TRT-LLM] [I] Starting TensorRT-LLM init.\n",
      "2025-07-30 10:39:36,410 [INFO] [TRT-LLM] [I] TensorRT-LLM inited.\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.10.1.dev2024053000\n",
      "INFO 07-30 10:39:37.235 api_server.py:489] NIM LLM API version 1.0.0\n",
      "INFO 07-30 10:39:37.236 ngc_profile.py:218] Running NIM without LoRA. Only looking for compatible profiles that do not support LoRA.\n",
      "INFO 07-30 10:39:37.236 ngc_profile.py:220] Detected 1 compatible profile(s).\n",
      "INFO 07-30 10:39:37.236 ngc_injector.py:107] Valid profile: 8835c31752fbc67ef658b20a9f78e056914fdef0660206d82f252d62fd96064d (vllm-fp16-tp1) on GPUs [0]\n",
      "INFO 07-30 10:39:37.236 ngc_injector.py:142] Selected profile: 8835c31752fbc67ef658b20a9f78e056914fdef0660206d82f252d62fd96064d (vllm-fp16-tp1)\n",
      "INFO 07-30 10:39:37.237 ngc_injector.py:147] Profile metadata: feat_lora: false\n",
      "INFO 07-30 10:39:37.237 ngc_injector.py:147] Profile metadata: llm_engine: vllm\n",
      "INFO 07-30 10:39:37.237 ngc_injector.py:147] Profile metadata: precision: fp16\n",
      "INFO 07-30 10:39:37.237 ngc_injector.py:147] Profile metadata: tp: 1\n",
      "INFO 07-30 10:39:37.237 ngc_injector.py:167] Preparing model workspace. This step might download additional files to run the model.\n",
      "INFO 07-30 10:39:37.238 ngc_injector.py:173] Model workspace is now ready. It took 0.001 seconds\n",
      "INFO 07-30 10:39:37.241 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='/tmp/meta--llama3-8b-instruct-f0d8e3a_', speculative_config=None, tokenizer='/tmp/meta--llama3-8b-instruct-f0d8e3a_', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "WARNING 07-30 10:39:37.526 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 07-30 10:39:37.543 utils.py:609] Found nccl from library /usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2\n",
      "INFO 07-30 10:39:38 selector.py:28] Using FlashAttention backend.\n",
      "INFO 07-30 10:39:42 model_runner.py:173] Loading model weights took 14.9595 GB\n",
      "INFO 07-30 10:39:43.244 gpu_executor.py:119] # GPU blocks: 27799, # CPU blocks: 2048\n",
      "INFO 07-30 10:39:44 model_runner.py:973] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 07-30 10:39:44 model_runner.py:977] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-30 10:39:49 model_runner.py:1054] Graph capturing finished in 5 secs.\n",
      "WARNING 07-30 10:39:49.957 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 07-30 10:39:49.969 serving_chat.py:347] Using default chat template:\n",
      "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "WARNING 07-30 10:39:50.227 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 07-30 10:39:50.239 api_server.py:456] Serving endpoints:\n",
      "  0.0.0.0:8000/openapi.json\n",
      "  0.0.0.0:8000/docs\n",
      "  0.0.0.0:8000/docs/oauth2-redirect\n",
      "  0.0.0.0:8000/metrics\n",
      "  0.0.0.0:8000/v1/health/ready\n",
      "  0.0.0.0:8000/v1/health/live\n",
      "  0.0.0.0:8000/v1/models\n",
      "  0.0.0.0:8000/v1/version\n",
      "  0.0.0.0:8000/v1/chat/completions\n",
      "  0.0.0.0:8000/v1/completions\n",
      "INFO 07-30 10:39:50.239 api_server.py:460] An example cURL request:\n",
      "curl -X 'POST' \\\n",
      "  'http://0.0.0.0:8000/v1/chat/completions' \\\n",
      "  -H 'accept: application/json' \\\n",
      "  -H 'Content-Type: application/json' \\\n",
      "  -d '{\n",
      "    \"model\": \"meta/llama3-8b-instruct\",\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"role\":\"user\",\n",
      "        \"content\":\"Hello! How are you?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\":\"assistant\",\n",
      "        \"content\":\"Hi! I am quite well, how can I help you today?\"\n",
      "      },\n",
      "      {\n",
      "        \"role\":\"user\",\n",
      "        \"content\":\"Can you write me a song?\"\n",
      "      }\n",
      "    ],\n",
      "    \"top_p\": 1,\n",
      "    \"n\": 1,\n",
      "    \"max_tokens\": 15,\n",
      "    \"stream\": true,\n",
      "    \"frequency_penalty\": 1.0,\n",
      "    \"stop\": [\"hello\"]\n",
      "  }'\n",
      "\n",
      "INFO 07-30 10:39:50.286 server.py:82] Started server process [74]\n",
      "INFO 07-30 10:39:50.287 on.py:48] Waiting for application startup.\n",
      "INFO 07-30 10:39:50.288 on.py:62] Application startup complete.\n",
      "INFO 07-30 10:39:50.289 server.py:214] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO 07-30 10:40:00.289 metrics.py:334] Avg prompt throughput: 0.3 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 07-30 10:40:10.289 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 07-30 10:40:20.289 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 07-30 10:40:30.289 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 07-30 10:40:40.289 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "!docker logs Llama3-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d448d-104d-4bce-b29e-84337a96fd9a",
   "metadata": {},
   "source": [
    "## Start Inferencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4efb19a8-63dc-413e-8e04-fe738664605c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in ./.local/lib/python3.10/site-packages (1.97.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.local/lib/python3.10/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: tqdm>4 in ./.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f06696d-a54f-49c1-a798-53c871671160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was a GPU so fine,\n",
      "Whose computing powers were truly divine.\n",
      "It processed with ease,\n",
      "Complex tasks with expertise,\n",
      "And solved problems in a short time!"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    # base_url='https://integrate.api.nvidia.com/v1',\n",
    "    base_url='http://0.0.0.0:8000/v1',\n",
    "    api_key=\"$NGC_API_KEY\"\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model='meta/llama3-8b-instruct',\n",
    "    messages=[{\"role\":\"user\", \"content\":\"Write a limerick about the wonders of GPU computing.\"}],\n",
    "    temperature=0.5,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab73cf0-cb0f-434e-a7ff-71f204f121ca",
   "metadata": {},
   "source": [
    "## Stop the Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34f54a0-334b-4033-9ede-5fc84dd72016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                       COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n",
      "036841bc33b1   nvcr.io/nim/meta/llama3-8b-instruct:1.0.3   \"/opt/nvidia/nvidia_â€¦\"   4 minutes ago   Up 4 minutes   0.0.0.0:8000->8000/tcp, :::8000->8000/tcp   Llama3-8B-Instruct\n",
      "Llama3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a && \\\n",
    "docker stop Llama3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a6fb4-0c7d-453e-bd5c-52a66c65b1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
